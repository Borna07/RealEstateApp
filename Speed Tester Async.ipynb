{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from django.utils import timezone\n",
    "\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import full\n",
    "from datetime import datetime\n",
    "pd.options.mode.chained_assignment = None \n",
    "import time\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def catch_links_all_pages(url):\n",
    "    oglasi = []\n",
    "    page1 = requests.get(url)\n",
    "    soup1 = BeautifulSoup(page1.content, \"html.parser\")\n",
    "\n",
    "    #Extract the number of pages from navigation bar\n",
    "    page_nav_class = soup1.find_all('ul', attrs={'class':\"pagination\"}) \n",
    "    page_nav = page_nav_class[0].find_all('li')\n",
    "    \n",
    "    \n",
    "    # #Take the last navigation element\n",
    "    # last = page_nav[-1]\n",
    "    # #Extract the number and clean data\n",
    "    # page_number = last.find('a').get('href')[-3:]\n",
    "    # page_number = re.sub(\"[^0-9]\", \"\", page_number)\n",
    "    # fix = \"&num=\"\n",
    "    \n",
    "    pages = []\n",
    "\n",
    "    #Check if there are multiple pages in the navigation\n",
    "    if not page_nav:\n",
    "\n",
    "        pages.append(url)\n",
    "\n",
    "    else:\n",
    "        #Take the last navigation element\n",
    "        last = page_nav[-1]\n",
    "        #Extract the number and clean data\n",
    "        page_number = last.find('a').get('href')[-3:]\n",
    "        page_number = re.sub(\"[^0-9]\", \"\", page_number)\n",
    "        fix = \"&num=\"\n",
    "        # print(page_number)\n",
    "\n",
    "        #Create a list of all pages with links\n",
    "        pages.append(url)\n",
    "        for i in range(2,int(page_number)+1):\n",
    "                link = url + fix + str(i)\n",
    "                pages.append(link)\n",
    "\n",
    "        pages = list(set(pages))\n",
    "       \n",
    "    #Scrape all resluts from every page\n",
    "    for page in pages:        \n",
    "        web_page = requests.get(page)\n",
    "        soup1 = BeautifulSoup(web_page.content, \"html.parser\")\n",
    "        results = soup1.find_all('div', attrs={'class':\"OglasiRezHolder\"}) \n",
    "        for result in results:\n",
    "            try:\n",
    "                link = result.find('a').get('href')\n",
    "                oglasi.append(link)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    oglasi = list(set(oglasi))\n",
    "\n",
    "    return oglasi\n",
    "    \n",
    "async def data_dict_index(url_list):\n",
    "    data_list = []\n",
    "    errors = []\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for url in url_list:\n",
    "            try:\n",
    "                async with session.get(url) as response:\n",
    "                    html = await response.text()\n",
    "                    doc = lxml.html.fromstring(html)\n",
    "\n",
    "                    my_dict = {}\n",
    "                    for row in doc.xpath(\"//li[@class='labela']\"):\n",
    "                        a = row.text_content()\n",
    "                        b = row.getnext().text_content().replace(\"\\r\\n\", \"\")\n",
    "                        if a in my_dict.keys():\n",
    "                            if b < my_dict[a]:\n",
    "                                my_dict[a] = b\n",
    "                        else:\n",
    "                            my_dict[a] = b\n",
    "                    \n",
    "                    my_dict[\"Cijena\"] = doc.xpath(\"//div[@class='price']/span\")[0].text_content()\n",
    "                    my_dict[\"Link\"] = url\n",
    "\n",
    "                    datum = doc.xpath(\"//div[@class='published']\")\n",
    "                    date = datum[1].text_content()\n",
    "                    my_dict[\"Datum\"] = date\n",
    "\n",
    "                    data_list.append(my_dict)\n",
    "\n",
    "            except:\n",
    "                errors.append(url)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def dataframe_cleaner(df):\n",
    "    #df = pd.DataFrame(data_list)\n",
    "\n",
    "    #Data Cleaning\n",
    "\n",
    "    #Drop rows w/o sqm, convert sqm to float\n",
    "    df = df[df['Stambena površina u m2'].notna()]\n",
    "    df[\"Stambena površina u m2\"] = df[\"Stambena površina u m2\"].str.replace(\",\",\".\")\n",
    "    df[\"Stambena površina u m2\"] = df[\"Stambena površina u m2\"].astype('float')\n",
    "\n",
    "    #Convert price to float, remove no price rows, rempve unrealistic low prices\n",
    "    df[\"Cijena\"] = df[\"Cijena\"].str.replace(\"€\",\"\")\n",
    "\n",
    "    df[\"Cijena\"] = df[\"Cijena\"].str.replace(\".\",\"\")\n",
    "    df[\"Cijena\"] = df[\"Cijena\"].str.replace(\",\",\".\")\n",
    "\n",
    "    df[\"Cijena\"] = df[\"Cijena\"].str.replace(\" \",\"\")\n",
    "\n",
    "    df = df[df[\"Cijena\"] != \"0,00 \"]\n",
    "    df[\"Cijena\"] = df[\"Cijena\"].astype('float')\n",
    "    df = df[df[\"Cijena\"] > 10000]\n",
    "\n",
    "    #Replace Nan with \"No info\"\n",
    "    df['Godina izgradnje'].fillna('No INFO', inplace=True)\n",
    "\n",
    "    #Remove where m2 == 0\n",
    "    df = df[df[\"Stambena površina u m2\"] > 10]\n",
    "    df = df[df[\"Stambena površina u m2\"] < 10000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Add a price per sqm meter column\n",
    "    df[\"€/m²\"] = round(df[\"Cijena\"] / df[\"Stambena površina u m2\"],2)\n",
    "\n",
    "    #Convert Datum to datetime\n",
    "    df[\"Datum\"] = pd.to_datetime(df[\"Datum\"], format=\"%d.%m.%Y\")\n",
    "\n",
    "    #Calculate days online\n",
    "    df['Days Online'] = ( pd.Timestamp('now') - df['Datum']).dt.days\n",
    "\n",
    "    return df\n",
    "\n",
    "async def main():\n",
    "    url_list = [\n",
    "        \"http://example.com/1\",\n",
    "        \"http://example.com/2\",\n",
    "        \"http://example.com/3\",\n",
    "    ]\n",
    "    data_list = await data_dict_index(url_list)\n",
    "    return data_list\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())\n",
    "loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catch_links_all_pages 0.7498210748036702 minutes\n",
      "data_dict_index: 0.7498210748036702 minutes\n",
      "<coroutine object data_dict_index at 0x00000199250CABC0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "\n",
    "url = \"https://www.index.hr/oglasi/prodaja-stanova/gid/3278?pojamZup=1166&tipoglasa=1&sortby=1&elementsNum=100&city=1638&naselje=0&cijenaod=0&cijenado=21000000&vezani_na=988-887_562-563_978-1334&num=1\"\n",
    "URL_rovinj = \"https://www.index.hr/oglasi/prodaja-stanova/gid/3278?pojam=&sortby=1&elementsNum=100&cijenaod=0&cijenado=21000000&tipoglasa=1&pojamZup=1154&city=1294&naselje=3374&attr_Int_988=&attr_Int_887=&attr_bit_stan=&attr_bit_brojEtaza=&attr_gr_93_1=&attr_gr_93_2=&attr_Int_978=&attr_Int_1334=&attr_bit_eneregetskiCertifikat=&vezani_na=988-887_562-563_978-1334\"\n",
    "URL_zg = \"https://www.index.hr/oglasi/prodaja-stanova/gid/3278?pojam=&sortby=1&elementsNum=100&cijenaod=0&cijenado=21000000&tipoglasa=1&pojamZup=1153&city=&naselje=&attr_Int_988=&attr_Int_887=&attr_bit_stan=&attr_bit_brojEtaza=&attr_gr_93_1=&attr_gr_93_2=&attr_Int_978=&attr_Int_1334=&attr_bit_eneregetskiCertifikat=&vezani_na=988-887_562-563_978-1334\"\n",
    "\n",
    "\n",
    "url = URL_zg\n",
    "city_name = \"Zagreb\"\n",
    "ime = \"_zagreb\"\n",
    "\n",
    "\n",
    "\n",
    "#find all pages\n",
    "pages = catch_links_all_pages(url)\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "elapsed_time_min = elapsed_time/60\n",
    "print('catch_links_all_pages', elapsed_time_min, 'minutes')\n",
    "\n",
    "\n",
    "#scrape all pages\n",
    "data_list = []\n",
    "\n",
    "data_list = data_dict_index(pages)\n",
    "\n",
    "# get the end time\n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "elapsed_time_min = elapsed_time/60\n",
    "print('data_dict_index:', elapsed_time_min, 'minutes')\n",
    "\n",
    "print(data_list)\n",
    "\n",
    "\n",
    "# #create dataframe\n",
    "# df = pd.DataFrame(data_list)\n",
    "# current_date = datetime.now().date()\n",
    "\n",
    "# #check raw entries, save raw dataframe\n",
    "# dj_raw_entries = len(df.index)\n",
    "\n",
    "# full_ime_raw = str(current_date) + \"_RAW\" + ime + \".xlsx\"\n",
    "# dj_df_raw  = df.to_excel(full_ime_raw)\n",
    "\n",
    "# #clean dataframe\n",
    "# df = dataframe_cleaner(df)\n",
    "\n",
    "# # get the end time\n",
    "# et = time.time()\n",
    "\n",
    "# # get the execution time\n",
    "# elapsed_time = et - st\n",
    "# elapsed_time_min = elapsed_time/60\n",
    "# print('dataframe_cleaner', elapsed_time_min, 'minutes')\n",
    "\n",
    "# full_ime =  str(current_date) + ime + \".xlsx\"\n",
    "\n",
    "# dj_df = df.to_excel(full_ime)\n",
    "# dj_clean_entries = len(df.index)\n",
    "\n",
    "# dj_avg_price_sqrm = round(df[\"€/m²\"].mean(),2)\n",
    "# dj_avg_size = round(df[\"Stambena površina u m2\"].mean(),2)\n",
    "# dj_avg_year = df[df[\"Godina izgradnje\"] != \"No INFO\"][\"Godina izgradnje\"].mode()[0]\n",
    "\n",
    "\n",
    "\n",
    "# max_df = df[df['Cijena'] == df['Cijena'].max()]\n",
    "# max = [max_df[\"Cijena\"].values[0], max_df[\"Link\"].values[0]]\n",
    "\n",
    "# max_per_sqr_df = df[df['€/m²'] == df['€/m²'].max()]\n",
    "# max_per_sqr = [max_per_sqr_df['€/m²'].values[0], max_per_sqr_df['Link'].values[0]]\n",
    "\n",
    "# min_df = df[df['Cijena'] == df['Cijena'].min()]\n",
    "# min = [min_df[\"Cijena\"].values[0], min_df[\"Link\"].values[0]]\n",
    "\n",
    "# min_per_sqr_df = df[df['€/m²'] == df['€/m²'].min()]\n",
    "# min_per_sqr = [min_per_sqr_df['€/m²'].values[0], min_per_sqr_df['Link'].values[0]]\n",
    "# med_sale_price = df[\"Cijena\"].mean()\n",
    "\n",
    "# # time_now = timezone.now()\n",
    "\n",
    "# #     new_entry = Document(\n",
    "# #     document = dj_df, document_raw = dj_df_raw, uploaded_at = time_now,\n",
    "# #     calendar_week = time_now.isocalendar().week , raw_entries = dj_raw_entries, clean_entries = dj_clean_entries,\n",
    "# #     city = city_name, avg_price_sqrm = dj_avg_price_sqrm, avg_size = dj_avg_size,\n",
    "# #     avg_year = dj_avg_year, highest_price = max[0], highest_price_link = max[1],\n",
    "# #     highest_price_sqrm = max_per_sqr[0], highest_price_sqrm_link = max_per_sqr[1],\n",
    "# #     lowest_price = min[0], lowest_price_link = min[1], lowest_price_sqrm = min_per_sqr[0],\n",
    "# #     lowest_price_sqrm_link = min_per_sqr[1]\n",
    "# # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # get the end time\n",
    "# et = time.time()\n",
    "\n",
    "# # get the execution time\n",
    "# elapsed_time = et - st\n",
    "# elapsed_time_min = elapsed_time/60\n",
    "# print('Execution time:', elapsed_time_min, 'minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_dict_index(url_list):\n",
    "\n",
    "    data_list = []\n",
    "    errors = []\n",
    "\n",
    "    for url in url_list:\n",
    "        try:\n",
    "\n",
    "            page = requests.get(url)    \n",
    "\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "            my_dict = {}\n",
    "            osnovno = soup.find_all('li', attrs={'class':'labela'})\n",
    "            for row in osnovno:\n",
    "                a = row.text\n",
    "                b = row.findNextSibling().get_text().replace(\"\\r\\n\", \"\")\n",
    "\n",
    "                if a in my_dict.keys():\n",
    "                    if b < my_dict[a]:\n",
    "                        my_dict[a] = b\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    my_dict[a] = b\n",
    "            \n",
    "            my_dict[\"Cijena\"] = soup.find('div', attrs={'class':'price'}).find('span').text\n",
    "            my_dict[\"Link\"] = url\n",
    "\n",
    "            datum = soup.find_all('div', attrs={'class':'published'})\n",
    "            regex = r\"\\d{2}.\\d{2}.\\d{4}\"\n",
    "            date = re.findall(regex,datum[1].get_text())[0]\n",
    "            my_dict[\"Datum\"] = date\n",
    "\n",
    "            data_list.append(data_dict_index(page))\n",
    "\n",
    "        except:\n",
    "            errors.append(url)\n",
    "\n",
    "    return data_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a926afa313b26ae1264fdcf81c726a97e69f6ba2ba780f6aa901948710f8d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
